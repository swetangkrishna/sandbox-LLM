🔍 LLM Sandbox Evaluation Framework

This repository provides a comprehensive framework to evaluate Large Language Models (LLMs) in a sandbox environment using structured test cases and well-defined metrics. The project includes visual analytics, performance summaries, and reproducible Jupyter notebooks for testing accuracy, completeness, and effectiveness.

📊 Project Overview

The aim of this repository is to:
	•	Evaluate LLMs against standardized test cases.
	•	Visualize performance using intuitive charts.
	•	Provide reproducible code and data for further benchmarking.

 📁 Repository Structure
├── sandbox.ipynb                # Notebook for running and analyzing sandbox evaluations \n
├── sandbox-LLM_testing.ipynb    # LLM-specific testing notebook with visual outputs \n
├── deepseek_llm_results.xlsx    # Evaluation scores for DeepSeek LLM \n
├── sandbox_results.xlsx         # Aggregated results from sandbox evaluations \n
├── sandbox-LLM.png              # Average metrics bar chart \n
├── sandbox-LLM-2.png            # Scatter plots for score distribution \n

📈 Visual Insights
	•	The bar chart represents average scores across 100 test cases.
	•	The scatter plots show per-test-case score distributions for each metric.

These visualizations help identify strengths and weaknesses in LLM performance.
