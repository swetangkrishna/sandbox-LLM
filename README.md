ğŸ” LLM Sandbox Evaluation Framework

This repository provides a comprehensive framework to evaluate Large Language Models (LLMs) in a sandbox environment using structured test cases and well-defined metrics. The project includes visual analytics, performance summaries, and reproducible Jupyter notebooks for testing accuracy, completeness, and effectiveness.

ğŸ“Š Project Overview

The aim of this repository is to:
	â€¢	Evaluate LLMs against standardized test cases.
	â€¢	Visualize performance using intuitive charts.
	â€¢	Provide reproducible code and data for further benchmarking.

 ğŸ“ Repository Structure
â”œâ”€â”€ sandbox.ipynb                # Notebook for running and analyzing sandbox evaluations \n
â”œâ”€â”€ sandbox-LLM_testing.ipynb    # LLM-specific testing notebook with visual outputs \n
â”œâ”€â”€ deepseek_llm_results.xlsx    # Evaluation scores for DeepSeek LLM \n
â”œâ”€â”€ sandbox_results.xlsx         # Aggregated results from sandbox evaluations \n
â”œâ”€â”€ sandbox-LLM.png              # Average metrics bar chart \n
â”œâ”€â”€ sandbox-LLM-2.png            # Scatter plots for score distribution \n

ğŸ“ˆ Visual Insights
	â€¢	The bar chart represents average scores across 100 test cases.
	â€¢	The scatter plots show per-test-case score distributions for each metric.

These visualizations help identify strengths and weaknesses in LLM performance.
